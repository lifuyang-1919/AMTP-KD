# AMTP-KD: Adaptive Multi-Teacher Pruned Knowledge Distillation  

ğŸš€ **[Project Under Development]** Code will be released soon. Stay tuned!  

## ğŸ“Œ Introduction  
High-performance LiDAR-based 3D object detectors often face significant computational overhead, necessitating their compression into lightweight detectors. Knowledge distillation (KD) is an effective approach for sparse 3D object detection compression.  

We propose **Adaptive Multi-Teacher Pruned Knowledge Distillation (AMTP-KD)**, which:  
- Generates **student-friendly teacher assistants** through structured and unstructured pruning.  
- Introduces a **sample-level teacher hm-loss-based fusion strategy**, incorporating a **hard selection strategy** and a **soft multi-stage Softmax-T strategy** for adaptive weighting.  
- Proposes **multi-teacher pivotal region masks** to enhance knowledge transfer.  

Extensive experiments on **Waymo & KITTI datasets** demonstrate that our method achieves a **4Ã— reduction in FLOPs and parameters** without noticeable accuracy loss in LiDAR-based 3D object detection.  

## ğŸ“– Features  
âœ… **Multi-Teacher Pruned Distillation**: Generates efficient teacher assistants via structured and unstructured pruning.  
âœ… **Adaptive Knowledge Fusion**: Incorporates hard selection and Softmax-T-based multi-stage strategies.  
âœ… **Multi-Teacher Pivotal Region Masks**: Enhances feature-level knowledge transfer.  

## ğŸ”§ Installation (Coming Soon)  
We will provide detailed installation instructions and dependencies once the code is uploaded.  

## ğŸš€ Usage (Coming Soon)  
Stay tuned for training and evaluation scripts!  

## ğŸ“… To-Do  
- [ ] Upload codebase  
- [ ] Release trained models   
- [ ] Provide detailed documentation  

%## ğŸ“¢ Stay Updated  
%â­ **Star** this repository to stay updated! We will release the code and models soon.  
